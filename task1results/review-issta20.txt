REVIEW:

Title: Detecting Flaky Tests in Probabilistic and Machine Learning Applications
Authors: Saikat Dutta, August Shi, Rutvik Choudhary, Zhekun Zhang, Aryaman Jain, 
Sasa Misailovic
Published in: ISSTA 2020 - 29th ACM SIGSOFT International Symposium on Software 
Testing and Analysis

Q: What is your take-away message from this paper?

A: Flaky tests are widely present in machine learning test suites due to the
randomness in almost every machine learning algorithm. It is possible to
systematically identify such tests by changing the random seeds with a 
dynamically calculated number of iterations. This mitigates the negative
effects of traditional flaky tests. 

Q: What is the motivation for this work (both people problem and technical 
problem), and its distillation into a research question? Why doesn’t the people 
problem have a trivial solution? What are the previous solutions and why are 
they inadequate?

A: This work is motivated by the past work in identifying and correcting flaky
tests and the simulatneous lack of a large scale study in areas specific to
machine learning and randmoization. The people problem isn't trivial for the
same reason as the previous paper, there isn't a one size fits all solution
to fixing flaky machine learning tests. There are no large scale previous
solutions in the specific area of machine learning. 

Q: What is the proposed solution (hypothesis, idea, design)? Why is it believed 
it will work? How does it represent an improvement? How is the solution 
achieved?

A: The proposed solution is FLASH, which detects "flaky tests due to
non-deterministic behavior of algorithms". It is believed it will work because
it was end-to-end tested from identifying flaky tests to correcting them and
submitting pull requests. This is an improvement from previous solutions
because it is systematic and specific to ML. The components of FLASH are
the Assertion Miner, Test Instrucmentor, Test Driver, and Assertion Inspector. 

Q: What is the author’s evaluation of the solution? What logic, argument, 
evidence, artifacts (e.g., a proof-of-concept system), or experiments are 
presented in support of the idea?

A: The authors evaluate metrics in section 4, specifically FLASH identifies
11 flaky tests, developers respond positively to the feedback, and FLASH
identifies 11 flaky tests from old code. 

Q: What is your analysis of the identified problem, idea and evaluation? Is this 
a good idea? What flaws do you perceive in the work? What are the most 
interesting or controversial ideas? For work that has practical implications, 
ask whether this will work, who would want it, what it will take to give it to 
them, and when might it become a reality?

A: I believe the identified problem is very relevant with the rise of LLM, 
especially with the age of transformers powering LLMs like ChatGPT. In terms
of the specific the research seems to follow from the previous paper, 
exploring the new area. The idea of developing a systematic end-to-end
methodology is good, and appears to lend itself to further automation. 
Similar to the previous papers, ML Engineers would want it, it would take
widespread distribution and usability for them to get it, and this might
become a reality in 2 years. 

Q: What are the paper’s contributions (author’s and your opinion)? Ideas, 
methods, software, experimental results, experimental techniques...?

A: The authors provide a concise list of contributions, specifically that
this is the first empirical study of probabilistic machine learning, the
development of the FLASH technology/methodology, and the evluation of FLASH. 
I agree with all of these contributions. I would further say that the pull
requests generated by the paper were a contribution, and that the 
mathematical techniques used to calculate the number of iterations are also. 

Q: What are future directions for this research (author’s and yours, perhaps 
driven by shortcomings or other critiques)?

A: One further direction suggested by the author is studying ways to improve
flaky tests in machine learning test suites not directly related to random
number generation (for example, concurrency was one big cause identified) in
the previous paper. Similar to that paper, one future direction would be to
massively scale up the research, by automating the process further; it seems
like the most time intensive part of the process currently is identifying
appopriate flaky tests to fix. 

Q: What questions are you left with? What questions would you like to raise in 
an open discussion of the work (review interesting and controversial points, 
above)? What do you find difficult to understand? List as many as you can, at
least three, not including questions that can be answered quickly by searching 
the internet.

A: I have several questions about the research. For one, I'm wondering
whether random number generation is the only characteristic of machine
learning algorithms that are associated with flaky tests or if there are
other characterisics conductive to flakiness. I also wonder about the
relationship between FLASH's implementation in Python and the implementation
of most machine learning libraries in Python. For example would FLASH be
less efficient/impossible if it were implemnted in another language. I'm also
wondering what the best way is to configure the mentioned parameters of FLASH 
such as "batch size, data set generation parameters (size and distribution), 
and number of reruns" or if any tool to do so if provided. 